experiment_name: "day8_whole_img_lr1-e5"
output_base_dir: "runs"
augmentation: null
training:
  images_dir: "dat/day_8_aug/images"
  labels_dir: "dat/day_8_aug/labels"
  model_type: "vit_b_lm"
  patch_shape: [2048, 2048]  # Full image size - no cropping/patching
  batch_size: 6  # Increased from 1 - plenty of GPU memory available (only using 13GB/80GB)
  n_epochs: 200  # Longer training since we see each image once per epoch
  n_samples: null  # null = use actual dataset size (all images, no random sampling)
  num_workers: 12  # Increased from 4 to utilize available CPUs (20 allocated)
  learning_rate: 1.0e-5
  early_stopping: 50  # Allow 100 epochs without improvement before stopping (weekend run can explore plateaus)
  val_split: 0.05
  shuffle_data: true
  shuffle_seed: 42
  train_instance_segmentation_only: false  # Train full SAM model
  use_min_instance_sampler: false  # Disabled - full images always have instances
  min_instances_per_patch: 1  # Not used when sampler disabled
  min_instance_size: 1  # Keep all objects â‰¥1 pixel (effectively no filtering)
  save_validation_predictions_frequency: 1
  checkpoint_name: "day_8_whole-img_1e-5_full_run"
  resume_from_checkpoint: null
  export_path: "final_models/day_8_whole-img_1e-5_full_run.pt"